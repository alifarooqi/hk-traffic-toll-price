# -*- coding: utf-8 -*-
"""HK Traffic.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12beD_XSA1gz7UIp4KuRoA368xyBeHXK1
"""

# ================================================
# Colab ETL: Historical Detector Data (Febâ€“Aug 2025)
# ================================================
import os, requests, xml.etree.ElementTree as ET, pandas as pd
from datetime import datetime, timedelta
from tqdm import tqdm

# -------------------------
# Step 1. Config
# -------------------------
START_DATE = datetime(2025, 5, 1)
END_DATE   = datetime(2025, 6, 30)
POLICY_CHANGE_DATE = datetime(2025, 5, 31)

LIST_API   = "https://api.data.gov.hk/v1/historical-archive/list-file-versions"
GET_API    = "https://api.data.gov.hk/v1/historical-archive/get-file"
RAW_URL    = "https://resource.data.one.gov.hk/td/traffic-detectors/rawSpeedVol-all.xml"

DATA_DIR   = "xml_cache"   # folder to store raw XML snapshots
CACHE_FILE = "hk_tunnel_traffic.csv"

LOCATIONS_CSV_URL = "https://static.data.gov.hk/td/traffic-data-strategic-major-roads/info/traffic_speed_volume_occ_info.csv"

os.makedirs(DATA_DIR, exist_ok=True)

# ================================================
# Step 2. Download all XML snapshots (resumable)
# ================================================
def download_snapshots():
    curr = START_DATE
    while curr <= END_DATE:
        day = curr.strftime("%Y%m%d")
        resp = requests.get(LIST_API, params={"url": RAW_URL, "start": day, "end": day})
        versions = resp.json().get("timestamps", [])
        print(f"{curr.date()} -> {len(versions)} snapshots")

        for ts in tqdm(versions):
            fname = os.path.join(DATA_DIR, f"{ts}.xml")
            if os.path.exists(fname):
                continue  # skip if already downloaded
            r = requests.get(GET_API, params={"url": RAW_URL, "time": ts})
            if r.status_code == 200:
                with open(fname, "wb") as f:
                    f.write(r.content)
        curr += timedelta(days=1)

print("ðŸ”½ Starting snapshot download...")
download_snapshots()
print("âœ… All snapshots downloaded to", DATA_DIR)

# ================================================
# Step 3. Load metadata & map corridors
# ================================================
loc_df = pd.read_csv(LOCATIONS_CSV_URL)
loc_df.columns = [c.strip() for c in loc_df.columns]

corridor_keywords = {
    "tai_lam": ["Tai Lam"],
    "nt_circular": ["Tuen Mun Road", "NT Circular"],
}

def map_corridor(road_name):
    for corridor, kws in corridor_keywords.items():
        for kw in kws:
            if kw.lower() in str(road_name).lower():
                return corridor
    return None

loc_df["corridor"] = loc_df["Road_EN"].apply(map_corridor)
loc_corridors = loc_df.dropna(subset=["corridor"])
valid_ids = set(loc_corridors["AID_ID_Number"].astype(str))

print("Corridors mapped:", loc_corridors["corridor"].unique())

# ================================================
# Step 4. Parse XML snapshots
# ================================================
def parse_snapshot_file(filepath):
    out = []
    try:
        tree = ET.parse(filepath)
        root = tree.getroot()
        date_text = root.findtext(".//date")
        for period in root.findall(".//period"):
            ts_str = f"{date_text} {period.findtext('period_from')}"
            for det in period.findall(".//detector"):
                did = det.findtext("detector_id")
                if did not in valid_ids:
                    continue

                speed, volume, occupancy = None, None, None
                if det.find("speed") is not None:
                    speed = float(det.findtext("speed") or 0)
                    volume = int(det.findtext("volume") or 0)
                    occupancy = float(det.findtext("occupancy") or 0)
                else:
                    lanes = det.findall(".//lane")
                    lane_speeds, lane_occ, lane_vols = [], [], []
                    for ln in lanes:
                        if ln.find("speed") is not None:
                            lane_speeds.append(float(ln.findtext("speed") or 0))
                        if ln.find("occupancy") is not None:
                            lane_occ.append(float(ln.findtext("occupancy") or 0))
                        if ln.find("volume") is not None:
                            lane_vols.append(int(ln.findtext("volume") or 0))
                    if lane_speeds:
                        speed = sum(lane_speeds) / len(lane_speeds)
                    if lane_occ:
                        occupancy = sum(lane_occ) / len(lane_occ)
                    if lane_vols:
                        volume = sum(lane_vols)

                out.append({
                    "timestamp": ts_str,
                    "detector_id": did,
                    "speed": speed,
                    "volume": volume,
                    "occupancy": occupancy
                })
    except Exception as e:
        return []
    return out

print("ðŸ“– Parsing XML snapshots...")
rows = []
for fname in tqdm(os.listdir(DATA_DIR)):
    filepath = os.path.join(DATA_DIR, fname)
    rows.extend(parse_snapshot_file(filepath))

print("Parsed rows:", len(rows))

# ================================================
# Step 5. Aggregate hourly by corridor
# ================================================
df = pd.DataFrame(rows)
df["timestamp"] = pd.to_datetime(df["timestamp"])

df = df.merge(loc_corridors[["AID_ID_Number","corridor"]],
              left_on="detector_id", right_on="AID_ID_Number", how="left")

df_hour = df.groupby([pd.Grouper(key="timestamp", freq="H"), "corridor"]).agg(
    volume=("volume","sum"),
    speed=("speed","mean"),
    occupancy=("occupancy","mean")
).reset_index()

pivot = df_hour.pivot(index="timestamp", columns="corridor", values="volume").fillna(0).reset_index()

# ================================================
# Step 6. Add buckets: before/after + peak/off-peak
# ================================================
pivot["period"] = pivot["timestamp"].apply(lambda x: "before" if x < POLICY_CHANGE_DATE else "after")

def is_peak(ts):
    hm = ts.hour*100 + ts.minute
    return ((715 <= hm <= 945) or (1715 <= hm <= 1900))

pivot["slot"] = pivot["timestamp"].apply(lambda x: "peak" if is_peak(x) else "offpeak" if x >= POLICY_CHANGE_DATE else "na")

pivot.to_csv(CACHE_FILE, index=False)
print("âœ… Aggregated data saved:", CACHE_FILE)
pivot.head()
